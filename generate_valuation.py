import requests
import pandas as pd
import numpy as np
import yfinance as yf
import json
import os
import time
from datetime import datetime

# --- 1. é…ç½® ---
FMP_API_KEY = os.getenv('FMP_API_KEY')
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = os.path.join(BASE_DIR, "data")
CACHE_BASE_DIR = os.path.join(OUTPUT_DIR, "fmp_cache") # ç·©å­˜ä¸»ç›®éŒ„
DOW_30 = [
    "AAPL", "TSLA", "AMZN", "MSFT", "NVDA", "GOOGL", "META", "NFLX", 
    "PYPL", "SOFI", "HOOD", "WMT", "GE", "CSCO", "JNJ", "CVX", "PLTR",
    "UNH",  "TSM", "DIS", "COST", "INTC", "KO", "TGT", "NKE", "BA", 
    "SHOP", "SBUX", "ADBE"
]

WINDOWS = {"1Y": 252, "2Y": 504, "3Y": 756, "5Y": 1260}
QUARTERS = ['q1', 'q2', 'q3', 'q4']

# --- 2. æŠ½å–å±¤ (Extract Layer) ---
def get_fmp_fragmented(endpoint, ticker):
    """
    [Data Engineering Logic]: 
    è‡ªå‹•å»ºç«‹å°æ‡‰ ticker çš„å­è³‡æ–™å¤¾ï¼Œä¸¦å¯¦æ–½ã€å¢é‡åˆä½µç­–ç•¥ã€ã€‚
    é˜²æ­¢æ–° API æ•¸æ“šè¦†è“‹æ‰èˆŠçš„æ­·å²è²¡å ±æ•¸æ“š (å°¤å…¶æ˜¯è§£æ±º FMP 5å¹´é™åˆ¶)ã€‚
    """
    combined_all_quarters = []
    
    # å»ºç«‹ ticker å°ˆå±¬è·¯å¾‘ï¼šdata/fmp_cache/{ticker}
    ticker_cache_dir = os.path.join(CACHE_BASE_DIR, ticker.upper())
    os.makedirs(ticker_cache_dir, exist_ok=True) 

    for q in QUARTERS:
        cache_path = os.path.join(ticker_cache_dir, f"{endpoint}_{q}.json")
        
        # 1. è®€å–ç¾æœ‰çš„ç·©å­˜æ•¸æ“š (å¦‚æœå­˜åœ¨)
        existing_data = []
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r') as f:
                    existing_data = json.load(f)
            except Exception as e:
                print(f"  âš ï¸ [Warning] Failed to load cache {cache_path}: {e}")
                existing_data = []

        # 2. æª¢æŸ¥æ˜¯å¦éœ€è¦ call API (7å¤©æœ‰æ•ˆæœŸ)
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæˆ–è€…å·²éæœŸï¼Œå‰‡ç™¼èµ·è«‹æ±‚
        is_expired = not os.path.exists(cache_path) or (time.time() - os.path.getmtime(cache_path)) > (7 * 86400)

        if is_expired:
            url = f"https://financialmodelingprep.com/stable/{endpoint}/?symbol={ticker}&period={q}&apikey={FMP_API_KEY}"
            try:
                print(f"  ğŸš€ [API Call] Fetching {ticker} {endpoint} {q} for incremental update...")
                res = requests.get(url).json()
                
                if isinstance(res, list) and len(res) > 0:
                    # --- æ ¸å¿ƒå¢é‡åˆä½µé‚è¼¯ ---
                    # A. å»ºç«‹ä¸€å€‹ä»¥æ—¥æœŸç‚º key çš„ dictionaryï¼Œå„ªå…ˆæ”¾å…¥ã€ŒèˆŠæ•¸æ“šã€
                    data_map = {item['date']: item for item in existing_data}
                    
                    # B. ç”¨ã€Œæ–°æ•¸æ“šã€å»æ›´æ–°/è¦†è“‹ç›¸åŒçš„æ—¥æœŸé» (ç¢ºä¿æœ€æ–°æ•¸æ“šæœ€æº–ç¢º)
                    # å¦‚æœæ˜¯èˆŠæ—¥æœŸ API æ²’å›å‚³ï¼Œå‰‡åŸæœ¬ data_map è£¡çš„èˆŠæ•¸æ“šæœƒè¢«ä¿ç•™
                    for item in res:
                        data_map[item['date']] = item
                    
                    # C. è½‰å›åˆ—è¡¨ä¸¦æŒ‰æ—¥æœŸæ’åº (ç”±æ–°åˆ°èˆŠ)
                    merged_res = sorted(data_map.values(), key=lambda x: x['date'], reverse=True)
                    
                    # D. å¯«å›æª”æ¡ˆ (é€™ç¾åœ¨åŒ…å«äº† 5 å¹´å‰çš„æ­·å² + å‰›æŠ“åˆ°çš„æ–°æ•¸æ“š)
                    with open(cache_path, 'w') as f:
                        json.dump(merged_res, f, indent=4)
                    
                    # å°‡åˆä½µå¾Œçš„çµæœåŠ å…¥æœ€çµ‚å›å‚³æ¸…å–®
                    combined_all_quarters.extend(merged_res)
                else:
                    # å¦‚æœ API æ²’å›å‚³æ–°æ•¸æ“šï¼Œè‡³å°‘ä¿ç•™èˆŠæ•¸æ“š
                    combined_all_quarters.extend(existing_data)
                    
                time.sleep(0.2)
            except Exception as e:
                print(f"  âŒ [Error] Failed to fetch {endpoint} {q}: {e}")
                combined_all_quarters.extend(existing_data)
        else:
            # 3. ç·©å­˜æœªéæœŸï¼Œç›´æ¥ä½¿ç”¨ç¾æœ‰çš„å®Œæ•´ç·©å­˜
            combined_all_quarters.extend(existing_data)
            
    return combined_all_quarters

# --- 3. è½‰æ›å±¤ (Transform Layer) ---
def build_quarterly_ttm(ticker):
    inc_list = get_fmp_fragmented("income-statement", ticker)
    cf_list = get_fmp_fragmented("cash-flow-statement", ticker)
    ev_list = get_fmp_fragmented("enterprise-values", ticker)

    if not all([inc_list, cf_list, ev_list]): return None, None

    df_inc = pd.DataFrame(inc_list).drop_duplicates('date').set_index('date').sort_index()
    df_cf = pd.DataFrame(cf_list).drop_duplicates('date').set_index('date').sort_index()
    df_ev = pd.DataFrame(ev_list).drop_duplicates('date').set_index('date').sort_index()

    for df in [df_inc, df_cf, df_ev]:
        df.index = pd.to_datetime(df.index).tz_localize(None)

    # --- é—œéµä¿®æ­£ï¼šè‡ªå‹•åµæ¸¬åŒ¯ç‡èˆ‡ ADR æ¯”ä¾‹ ---
    currency = df_inc['reportedCurrency'].iloc[-1] if 'reportedCurrency' in df_inc.columns else "USD"
    fx_rate = 32.5 if currency == "TWD" else 1.0  # å°ç©é›»æ•¸æ“šé€šå¸¸æ˜¯ TWD
    adr_ratio = 5.0 if ticker.upper() == "TSM" else 1.0 # 1 TSM = 5 è‚¡æ™®é€šè‚¡

    # --- è¨ˆç®— P/S å¿…å‚™çš„ Revenue TTM ---
    # å…ˆè¨ˆç®—æ¯å­£åº¦çš„ Sales Per Share
    # æ³¨æ„ï¼šRevenue åœ¨ income-statementï¼ŒnumberOfShares åœ¨ enterprise-values
    df_main = pd.concat([
        df_inc[['eps', 'revenue','netIncome']], 
        df_cf['freeCashFlow'], 
        df_ev['numberOfShares']
    ], axis=1).ffill()
    
    # çµ±ä¸€ä½¿ç”¨ç¸½é¡é™¤ä»¥ (ç¸½è‚¡æ•¸/ADRæ¯”ä¾‹) å†é™¤ä»¥åŒ¯ç‡
    # é€™æ¨£ç®—å‡ºä¾†æ‰æ˜¯ã€Œæ¯ä¸€å–®ä½ç¾é‡‘ ADRã€å°æ‡‰çš„åƒ¹å€¼
    # è¨ˆç®—æ¯è‚¡ç‡Ÿæ”¶ (Sales Per Share)
    
    df_main['sales_ps_adj'] = (df_main['revenue'] / df_main['numberOfShares'] ) / fx_rate
    df_main['eps_adj'] = (df_main['netIncome'] / df_main['numberOfShares'] ) / fx_rate
    df_main['fcf_ps_adj'] = (df_main['freeCashFlow'] / df_main['numberOfShares'] ) / fx_rate

    # Set to None to display all columns
    # pd.set_option('display.max_columns', None)

    # # Prevents the dataframe from wrapping to a new line
    # pd.set_option('display.expand_frame_repr', False)
    
    # è¨ˆç®— TTM (æ»¾å‹•å››å€‹å­£åº¦ç¸½å’Œ)
    df_main['eps_ttm'] = df_main['eps_adj'].rolling(window=4).sum()
    df_main['fcf_ps_ttm'] = df_main['fcf_ps_adj'].rolling(window=4).sum()
    df_main['sales_ps_ttm'] = df_main['sales_ps_adj'].rolling(window=4).sum()


    return (
        df_main[['eps_ttm']].dropna(), 
        df_main[['fcf_ps_ttm']].dropna(), 
        df_main[['sales_ps_ttm']].dropna()
    )

# --- 3. æ ¸å¿ƒä¼°å€¼é‚è¼¯ (Senior Analyst Hybrid Version) ---
def calculate_bands(ticker, prices_df, metrics_df, col_name):
    # æ—¥æœŸæ¨™æº–åŒ–èˆ‡å…¨æ™‚é–“è»¸åˆä½µ
    prices_df.index = pd.to_datetime(prices_df.index).tz_localize(None).normalize()
    metrics_df.index = pd.to_datetime(metrics_df.index).tz_localize(None).normalize()
    
    all_dates = prices_df.index.union(metrics_df.index).sort_values()
    df = pd.DataFrame(index=all_dates).join(prices_df)
    df['metric_raw'] = metrics_df[col_name]

    # è™•ç†æ‹†åˆ†èª¿æ•´å› å­ (å³ä½¿ yfinance èª¿æ•´éï¼Œæ­¤è™•ä»ä¿ç•™é‚è¼¯ä»¥é˜²è¬ä¸€)
    df['adj_ratio'] = (df['Adj Close'] / df['Close'].replace(0, np.nan)).ffill().bfill()
    df['metric_adj'] = df['metric_raw'] * df['adj_ratio']
    df['metric_final'] = df['metric_adj'].interpolate(method='time').ffill().bfill()

    # è¨ˆç®—å€æ•¸ï¼šæ’é™¤è² å€¼
    df['multiple'] = df['Adj Close'] / df['metric_final']
    df.loc[df['metric_final'] <= 0, 'multiple'] = np.nan

    # --- ç­–ç•¥é¸æ“‡é‚è¼¯ ---
    # å¦‚æœè² å€¼æˆ–æ¥µç«¯å€¼æ¯”ä¾‹éé«˜ (å¦‚ AMZN)ï¼Œè‡ªå‹•åˆ‡æ›è‡³ Median
    null_ratio = df['multiple'].isna().mean()
    use_median = True if (ticker == "AMZN" or null_ratio > 0.1) else False
    
    # 3. ã€æ ¸å¿ƒä¿®æ­£ã€‘ç™¾åˆ†ä½å‰ªæ (Percentile Approach)
    # æˆ‘å€‘è¨ˆç®—è©²è‚¡ç¥¨æ­·å²ä¸Š 90% åˆ†ä½æ•¸çš„å€¼ä½œç‚ºä¸Šé™
    # é€™æ¨£ AMZN çš„ 1000x æœƒè¢«å‰ªæ‰ï¼Œä½† AAPL çš„ 35x æœƒè¢«å®Œæ•´ä¿ç•™
    if df['multiple'].notna().any():
        upper_limit = df['multiple'].quantile(0.95)
        lower_limit = df['multiple'].quantile(0.05)
        df['multiple'] = df['multiple'].clip(lower=lower_limit, upper=upper_limit)

    results = {}
    avgs = {}

    for label, window in WINDOWS.items():
        # Hybrid æ»¾å‹•è¨ˆç®—
        if use_median:
            m_col = df['multiple'].rolling(window=window, min_periods=60).median()
        else:
            m_col = df['multiple'].rolling(window=window, min_periods=60).mean()
            
        s_col = df['multiple'].rolling(window=window, min_periods=60).std().fillna(0)
        
        # é˜²æ­¢æ¨™æº–å·®éå¤§å°è‡´ Band ç‚¸é–‹ (ä¸Šé™è¨­ç‚ºå‡å€¼çš„ 50%)
        s_col = s_col.clip(upper=m_col * 0.5)

        res = pd.DataFrame(index=df.index)
        res['mean'] = m_col * df['metric_final']
        res['up1'] = (m_col + s_col) * df['metric_final']
        res['up2'] = (m_col + 2 * s_col) * df['metric_final']
        res['down1'] = (m_col - s_col) * df['metric_final']
        res['down2'] = (m_col - 2 * s_col) * df['metric_final']

        # å¼·åˆ¶æ­¸é›¶é‚è¼¯ï¼šæŒ‡æ¨™ç‚ºè² å‰‡ä¼°å€¼ç‚º 0
        for c in res.columns:
            res.loc[df['metric_final'] <= 0, c] = 0

        results[label] = res.loc[prices_df.index].clip(lower=0).ffill().round(2)
        
        last_val = m_col.dropna().iloc[-1] if not m_col.dropna().empty else 0
        avgs[label] = round(float(last_val), 2)



    return results, avgs

def clean_nans(obj):
    if isinstance(obj, dict):
        return {k: clean_nans(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_nans(v) for v in obj]
    elif isinstance(obj, float) and np.isnan(obj):
        return None # JSON æ”¯æ´ nullï¼Œä¸æ”¯æ´ NaN
    return obj

# --- 5. ä¸»ç¨‹åº ---
def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # å‘¼å« Debug
    ## test_amzn_valuation_logic()

    for ticker in DOW_30:
        # 1. ç²å–è‚¡åƒ¹æ•¸æ“š
        # æˆ‘å€‘ä½¿ç”¨ auto_adjust=False ä»¥æ‰‹å‹•è™•ç† Close/Adj Close ä¾†å°é½ŠæŒ‡æ¨™é‡ç´š
        print(f"\nğŸ—ï¸  Pipeline Starting: {ticker}")
        prices = yf.Ticker(ticker).history(period="10y", auto_adjust=False)

        if prices.empty:
            print(f"  âš ï¸ [Skip] No price data for {ticker}")
            continue

        prices.index = prices.index.tz_localize(None)

        prices_df = prices[['Close', 'Adj Close']].copy()

        # 2. ç²å–è²¡å‹™æŒ‡æ¨™æ•¸æ“š (TTM)
        # ç¾åœ¨ build_quarterly_ttm æœƒå›å‚³ä¸‰å€‹æŒ‡æ¨™
        eps_ttm, fcf_ttm, sales_ttm = build_quarterly_ttm(ticker)
        if eps_ttm is None: continue

        pe_res, pe_avgs = calculate_bands(ticker, prices_df, eps_ttm, 'eps_ttm')
        fcf_res, fcf_avgs = calculate_bands(ticker, prices_df, fcf_ttm, 'fcf_ps_ttm')
        ps_res, ps_avgs = calculate_bands(ticker, prices_df, sales_ttm, 'sales_ps_ttm')
        
        # 4. å°è£æ­·å²æ•¸æ“šç”¨æ–¼å‰ç«¯ç¹ªåœ–
        history = []
        # åªå– 2021 å¹´ä»¥å¾Œçš„æ•¸æ“šé»ä»¥å„ªåŒ–å‰ç«¯åŠ è¼‰é€Ÿåº¦
        plot_df = prices_df[prices_df.index >= '2021-01-01']
        plot_df.index = plot_df.index.tz_localize(None).normalize()

        for date, row in plot_df.iterrows():
            # ç¢ºä¿è©²æ—¥æœŸåœ¨æ‰€æœ‰æŒ‡æ¨™è¨ˆç®—çµæœä¸­éƒ½å­˜åœ¨
            if date not in pe_res["1Y"].index: continue
            history.append({
                "date": date.strftime("%Y-%m-%d"),
                "price": round(float(row['Adj Close']), 2),
                "valuation": {
                    lb: {
                        "pe": pe_res[lb].loc[date].round(2).to_dict(),
                        "fcf": fcf_res[lb].loc[date].round(2).to_dict(),
                        "ps": ps_res[lb].loc[date].to_dict()   # åŠ å…¥ P/S
                    } for lb in WINDOWS
                }
            })
        # --- æ›´æ–° JSON çµæ§‹ï¼ŒåŠ å…¥ last_updated ---
        output_data = {
            "ticker": ticker.upper(), 
            "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),  # åŠ å…¥é€™è¡Œ
            "averages": {
                "pe": pe_avgs, 
                "fcf": fcf_avgs,
                "ps": ps_avgs
            }, 
            "data": history
        }

        # æœ€å¾Œçµæœä¹Ÿå­˜å…¥ ticker è³‡æ–™å¤¾
        final_dir = os.path.join(OUTPUT_DIR, "results", ticker.upper())
        os.makedirs(final_dir, exist_ok=True)
        
        with open(os.path.join(final_dir, "valuation_summary.json"), "w") as f:
            json.dump(clean_nans(output_data), f, indent=4)
        print(f"âœ¨ [Success] {ticker} pipeline execution completed. Folder: {final_dir} {len(history)} points generated.")

if __name__ == "__main__":
    main()
