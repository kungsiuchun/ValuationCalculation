import requests
import pandas as pd
import numpy as np
import yfinance as yf
import json
import os
import time
from datetime import datetime

# --- 1. é…ç½® ---
FMP_API_KEY = "F9dROu64FwpDqETGsu1relweBEoTcpID"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = os.path.join(BASE_DIR, "data")
CACHE_BASE_DIR = os.path.join(OUTPUT_DIR, "fmp_cache") # ç·©å­˜ä¸»ç›®éŒ„
DOW_30 = ["AMZN", "AAPL", "GOOGL", "MSFT", "WMT"] 

WINDOWS = {"1Y": 252, "2Y": 504, "3Y": 756, "5Y": 1260}
QUARTERS = ['q1', 'q2', 'q3', 'q4']

# --- 2. æŠ½å–å±¤ (Extract Layer) ---
def get_fmp_fragmented(endpoint, ticker):
    """
    [Data Engineering Logic]: 
    è‡ªå‹•å»ºç«‹å°æ‡‰ ticker çš„å­è³‡æ–™å¤¾ (ä¾‹å¦‚ fmp_cache/AMZN/)ã€‚
    """
    combined = []
    
    # å»ºç«‹ ticker å°ˆå±¬è·¯å¾‘ï¼šdata/fmp_cache/{ticker}
    ticker_cache_dir = os.path.join(CACHE_BASE_DIR, ticker.upper())
    os.makedirs(ticker_cache_dir, exist_ok=True) # è‡ªå‹•å»ºç«‹å¤šå±¤ç›®éŒ„

    for q in QUARTERS:
        # æ–‡ä»¶å‘½åä¿æŒ endpoint å€åˆ†
        cache_path = os.path.join(ticker_cache_dir, f"{endpoint}_{q}.json")
        
        # ç·©å­˜æª¢æŸ¥ (7å¤©æœ‰æ•ˆæœŸ)
        if os.path.exists(cache_path) and (time.time() - os.path.getmtime(cache_path)) < (7 * 86400):
            with open(cache_path, 'r') as f:
                combined.extend(json.load(f))
            continue

        url = f"https://financialmodelingprep.com/stable/{endpoint}/?symbol={ticker}&period={q}&apikey={FMP_API_KEY}"
        
        try:
            print(f"  ğŸš€ [API Call] Fetching {ticker} {endpoint} {q}...")
            res = requests.get(url).json()
            if isinstance(res, list):
                with open(cache_path, 'w') as f:
                    json.dump(res, f, indent=4) # å¢åŠ  indent æ–¹ä¾¿ DE é€²è¡Œ Debug
                combined.extend(res)
            time.sleep(0.2)
        except Exception as e:
            print(f"  âŒ [Error] Failed to fetch {endpoint} {q}: {e}")
            
    return combined

# --- 3. è½‰æ›å±¤ (Transform Layer) ---
def build_quarterly_ttm(ticker):
    inc_list = get_fmp_fragmented("income-statement", ticker)
    cf_list = get_fmp_fragmented("cash-flow-statement", ticker)
    ev_list = get_fmp_fragmented("enterprise-values", ticker)
    
    if not all([inc_list, cf_list, ev_list]): return None, None

    df_inc = pd.DataFrame(inc_list).drop_duplicates('date').set_index('date').sort_index()
    df_cf = pd.DataFrame(cf_list).drop_duplicates('date').set_index('date').sort_index()
    df_ev = pd.DataFrame(ev_list).drop_duplicates('date').set_index('date').sort_index()

    for df in [df_inc, df_cf, df_ev]:
        df.index = pd.to_datetime(df.index).tz_localize(None)

    # æ•¸æ“šåˆä½µèˆ‡è¨ˆç®— TTM
    df_inc['eps_ttm'] = df_inc['eps'].rolling(window=4).sum()
    df_main = pd.concat([df_inc[['eps_ttm']], df_cf['freeCashFlow'], df_ev['numberOfShares']], axis=1).ffill()
    df_main['fcf_ps_ttm'] = (df_main['freeCashFlow'] / df_main['numberOfShares']).rolling(window=4).sum()

    return df_main[['eps_ttm']].dropna(), df_main[['fcf_ps_ttm']].dropna()

def calculate_bands(ticker, prices, metrics_df, col_name):
    """
    [Data Engineering Logic]:
    1. å¯¦æ–½å‹•æ…‹æ‹†åˆ†èª¿æ•´ (Fixed NameError)
    2. ä½¿ç”¨ Rolling Mean å–ä»£ Median ä»¥ç²å¾—æ›´å¥½çš„å¹³æ»‘åº¦
    3. å¯¦æ–½å‹•æ…‹ç¸®å°¾è™•ç† (Winsorization) æ¶ˆé™¤é›¢ç¾¤å€¼
    """
    tk = yf.Ticker(ticker)
    adj_metrics = metrics_df.copy()
    
    # --- 1. æ‹†åˆ†èª¿æ•´ (ä¿®å¾© splits æœªå®šç¾©å•é¡Œ) ---
    try:
        # æ­£ç¢ºå®šç¾© splits
        splits = tk.splits
        if not splits.empty:
            for split_date, ratio in splits.items():
                split_date_naive = split_date.tz_localize(None)
                # æ­·å²è²¡å ±æ•¸æ“šéœ€èˆ‡èª¿æ•´å¾Œè‚¡åƒ¹å°é½Š
                adj_metrics.loc[adj_metrics.index < split_date_naive, col_name] /= ratio
    except Exception as e:
        print(f"  âš ï¸ [Warning] Could not process splits for {ticker}: {e}")

    # --- 2. æ•¸æ“šå°é½Šèˆ‡å¡«å…… (è§£æ±º 0.0 èˆ‡ NaN) ---
    df = pd.concat([prices, adj_metrics], axis=1).sort_index()
    # ä½¿ç”¨ bfill() ç¢ºä¿æ™‚é–“åºåˆ—é–‹é ­ä¸ç‚ºç©ºï¼Œå†é€²è¡Œç·šæ€§æ’å€¼
    df['val_smooth'] = df[col_name].ffill().bfill().interpolate(method='time').ffill().bfill()

    # --- 3. è¨ˆç®—åŸå§‹å€æ•¸ (è™•ç† AMZN è²  FCF å•é¡Œ) ---
    # åªæœ‰ç•¶æŒ‡æ¨™ > 0 æ™‚è¨ˆç®—å€æ•¸ï¼Œå¦å‰‡è¨­ç‚º NaN éš¨å¾Œå¡«å……ï¼Œç¢ºä¿å€æ•¸æ†æ­£
    df['raw_mult'] = np.where(df['val_smooth'] > 1e-4, df['Close'] / df['val_smooth'], np.nan)
    df['mult_filled'] = df['raw_mult'].ffill().bfill()

    # --- 4. Winsorization (ç¸®å°¾è™•ç†)ï¼šç¢ºä¿ Rolling Mean ä¸è¢«æ±¡æŸ“ ---
    # å‹•æ…‹è¨ˆç®—è©²è‚¡ç¥¨è‡ªèº«çš„ 15% èˆ‡ 85% åˆ†ä½æ•¸ä½œç‚ºé‚Šç•Œ
    q_low = df['mult_filled'].quantile(0.15)
    q_high = df['mult_filled'].quantile(0.85)
    df['mult_capped'] = df['mult_filled'].clip(lower=q_low, upper=q_high)

    results = {}
    avgs = {}

    for label, window in WINDOWS.items():
        # --- 5. Rolling Mean è¨ˆç®— (æ‡‰è¦æ±‚å–ä»£ Median) ---
        # min_periods=1 ç¢ºä¿å¾ç¬¬ä¸€å¤©é–‹å§‹å°±æœ‰æ•¸æ“šï¼Œæ¶ˆæ»… NaN
        m_col = df['mult_capped'].rolling(window=window, min_periods=1).mean().ffill().bfill()
        s_col = df['mult_capped'].rolling(window=window, min_periods=1).std().fillna(0).ffill().bfill()
        
        # é™åˆ¶ä¼°å€¼å¸¶æ¨™æº–å·®ç¯„åœ (Volatility Cap)
        s_col = np.minimum(s_col, m_col * 0.2)

        res = pd.DataFrame(index=df.index)
        # ç¢ºä¿æŒ‡æ¨™åŸºæº–ç‚ºæ­£ï¼Œé˜²æ­¢ mean è®Šè² 
        v_base = df['val_smooth'].clip(lower=0.01)
        
        # --- 6. ç”Ÿæˆæœ€å¾Œçµæœ ---
        res['mean'] = m_col * v_base
        res['up1'] = (m_col + s_col) * v_base
        res['up2'] = (m_col + 2 * s_col) * v_base
        res['down1'] = (m_col - s_col) * v_base
        res['down2'] = (m_col - 2 * s_col) * v_base
        
        # æ ¼å¼åŒ–è¼¸å‡º
        final_df = res.clip(lower=0.01).round(2)
        results[label] = final_df.replace([np.inf, -np.inf], 0).fillna(0)
        avgs[label] = round(float(m_col.iloc[-1]), 2)

    return results, avgs

# --- 5. ä¸»ç¨‹åº ---
def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    for ticker in DOW_30:
        print(f"\nğŸ—ï¸  Pipeline Starting: {ticker}")
        prices = yf.Ticker(ticker).history(period="7y")[['Close']]
        prices.index = prices.index.tz_localize(None)

        eps_ttm, fcf_ttm = build_quarterly_ttm(ticker)
        if eps_ttm is None: continue

        pe_res, pe_avgs = calculate_bands(ticker, prices['Close'], eps_ttm, 'eps_ttm')
        fcf_res, fcf_avgs = calculate_bands(ticker, prices['Close'], fcf_ttm, 'fcf_ps_ttm')

        history = []
        for date, row in prices[prices.index >= '2021-01-01'].iterrows():
            if date not in pe_res["1Y"].index: continue
            history.append({
                "date": date.strftime("%Y-%m-%d"),
                "price": round(row['Close'], 2),
                "valuation": {
                    lb: {
                        "pe": pe_res[lb].loc[date].round(2).to_dict(),
                        "fcf": fcf_res[lb].loc[date].round(2).to_dict()
                    } for lb in WINDOWS
                }
            })

        # æœ€å¾Œçµæœä¹Ÿå­˜å…¥ ticker è³‡æ–™å¤¾
        final_dir = os.path.join(OUTPUT_DIR, "results", ticker.upper())
        os.makedirs(final_dir, exist_ok=True)
        
        with open(os.path.join(final_dir, "valuation_summary.json"), "w") as f:
            json.dump({"ticker": ticker, "averages": {"pe": pe_avgs, "fcf": fcf_avgs}, "data": history}, f, indent=4)
        print(f"âœ¨ [Success] {ticker} pipeline execution completed. Folder: {final_dir}")

if __name__ == "__main__":
    main()