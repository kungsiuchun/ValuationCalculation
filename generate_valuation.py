import requests
import pandas as pd
import numpy as np
import yfinance as yf
import json
import os
from datetime import datetime, timedelta

# --- 1. åˆå§‹åŒ–èˆ‡é…ç½® ---
FMP_API_KEY = "F9dROu64FwpDqETGsu1relweBEoTcpID"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = os.path.join(BASE_DIR, "data")
DOW_30 = ["AMZN"]
## ["AAPL", "MSFT", "WMT", "GOOGL", "AMZN"] 

# å®šç¾©æ»¾å‹•é€±æœŸï¼ˆä»¥äº¤æ˜“æ—¥è¨ˆç®—ï¼Œä¸€å¹´ç´„ 252 å¤©ï¼‰
WINDOWS = {
    "1Y": 252,
    "2Y": 504,
    "3Y": 756,
    "5Y": 1260
}

def load_local_data(ticker):
    """ æª¢æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰ JSON æª”ï¼Œç”¨æ–¼å¢é‡åˆ¤æ–· """
    file_path = os.path.join(OUTPUT_DIR, f"{ticker}_valuation.json")
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"  âš ï¸  Warning: Local JSON for {ticker} exists but is corrupted: {e}")
            return None
    return None

def get_income_statement(ticker):
    """ å¾ FMP ç²å–åˆ©æ½¤è¡¨ (EPS æ•¸æ“š) """
    url = f"https://financialmodelingprep.com/stable/income-statement?symbol={ticker}&apikey={FMP_API_KEY}"
    try:
        res = requests.get(url).json()
        if not res or "Error" in str(res): 
            print(f"  âŒ FMP API Error (Income Statement): {res}")
            return None
        df = pd.DataFrame(res)[['date', 'eps']]
        df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)
        return df.set_index('date').sort_index()
    except Exception as e:
        print(f"  âŒ Failed to fetch Income Statement for {ticker}: {e}")
        return None

def get_cash_flow_statement(ticker):
    """ å¾ FMP ç²å–ç¾é‡‘æµé‡è¡¨èˆ‡æ­·å²è‚¡æ•¸ï¼Œè¨ˆç®—æ¯è‚¡ FCF """
    url_cf = f"https://financialmodelingprep.com/stable/cash-flow-statement?symbol={ticker}&apikey={FMP_API_KEY}"
    url_ev = f"https://financialmodelingprep.com/stable/enterprise-values/?symbol={ticker}&apikey={FMP_API_KEY}"
    try:
        # æŠ“å–ç¾é‡‘æµ
        res_cf = requests.get(url_cf).json()
        df_cf = pd.DataFrame(res_cf)[['date', 'freeCashFlow']]
        df_cf['date'] = pd.to_datetime(df_cf['date']).dt.tz_localize(None)
        df_cf = df_cf.set_index('date').sort_index()

        # æŠ“å–è‚¡æ•¸ (ç”¨æ–¼è¨ˆç®— Per Share æ•¸æ“š)
        res_ev = requests.get(url_ev).json()
        df_ev = pd.DataFrame(res_ev)[['date', 'numberOfShares']]
        df_ev['date'] = pd.to_datetime(df_ev['date']).dt.tz_localize(None)
        df_ev = df_ev.set_index('date').sort_index()

        # å°‡å…©ä»½å ±è¡¨å°é½Š
        df_merge = pd.merge_asof(df_cf, df_ev, left_index=True, right_index=True, direction='nearest', tolerance=pd.Timedelta(days=30))
        df_merge['fcf_ps'] = df_merge['freeCashFlow'] / df_merge['numberOfShares']
        return df_merge[['fcf_ps']]
    except Exception as e:
        print(f"  âŒ Failed to fetch Cash Flow Statement for {ticker}: {e}")
        return None

def calculate_multi_period_bands(ticker, price_series, metric_series, metric_name):
    """ 
    ä¿®æ­£ç‰ˆæ ¸å¿ƒç®—æ³•ï¼š
    1. ç²å–è‚¡ç¥¨æ‹†åˆ†æ­·å²ä¸¦èª¿æ•´è²¡å‹™æ•¸æ“š (è§£æ±º AMZN/GOOGL æ‹†åˆ†å°è‡´çš„ä¼°å€¼æ–·å±¤)
    2. å°‡è‚¡åƒ¹èˆ‡èª¿æ•´å¾Œçš„è²¡å‹™æ•¸æ“šå°é½Šä¸¦ç·šæ€§æ’å€¼
    3. è¨ˆç®—æ­·å²æ»¾å‹• PE/FCF å€æ•¸ (Multiple)
    4. ç”Ÿæˆ 1Y, 2Y, 3Y, 5Y çš„ 5 æ¢ä¼°å€¼é€šé“ç·š
    """
    # --- Step A: è‚¡ç¥¨æ‹†åˆ†èª¿æ•´ (ä¿æŒä¸è®Š) ---
    tk = yf.Ticker(ticker)
    splits = tk.splits
    adjusted_metric = metric_series.copy()
    if not splits.empty:
        for split_date, ratio in splits.items():
            split_dt = split_date.tz_localize(None)
            adjusted_metric.loc[adjusted_metric.index < split_dt] /= ratio

    # --- Step B: æ•¸æ“šå°é½Š ---
    combined = pd.concat([price_series, adjusted_metric], axis=1).sort_index()
    combined[f'{metric_name}_smooth'] = combined[metric_name].interpolate(method='time').ffill().bfill()
    df = combined.dropna(subset=['Close']).copy()
    
    # --- Step C: è¨ˆç®—å€æ•¸ (ä¿®æ­£è² æ•¸å•é¡Œ) ---
    # å¦‚æœè²¡å‹™æŒ‡æ¨™ç‚ºè² (å¦‚ FCF < 0)ï¼Œè©²å¤©çš„ Multiple è¨­ç‚º NaNï¼Œä¸åƒèˆ‡æ»¾å‹•å¹³å‡è¨ˆç®—
    df['multiple'] = df['Close'] / df[f'{metric_name}_smooth']
    df.loc[df[f'{metric_name}_smooth'] <= 0, 'multiple'] = np.nan 

    period_results = {}
    current_averages = {}

    for label, window_size in WINDOWS.items():
        # è¨ˆç®—æ»¾å‹•å‡å€¼ï¼Œè·³é NaN (å³è·³éè²  FCF çš„æ™‚æœŸ)
        # å¢åŠ  min_periods è¦æ±‚ï¼Œä¾‹å¦‚è‡³å°‘è¦æœ‰è©²çª—å£ 20% çš„æœ‰æ•ˆæ•¸æ“šï¼Œå¦å‰‡ä¸é¡¯ç¤ºï¼Œé¿å…æ•¸æ“šå‰›é–‹å§‹æ™‚éåº¦é‡åˆ
        df[f'mean_{label}'] = df['multiple'].rolling(window=window_size, min_periods=max(1, int(window_size*0.1))).mean()
        df[f'std_{label}'] = df['multiple'].rolling(window=window_size, min_periods=max(1, int(window_size*0.1))).std().fillna(0)

        bands = pd.DataFrame(index=df.index)
        m_col = df[f'mean_{label}']
        s_col = df[f'std_{label}']
        val_col = df[f'{metric_name}_smooth']

        # ç”Ÿæˆä¼°å€¼ç·š (æ³¨æ„ï¼šå³ä½¿ Multiple æ˜¯ NaNï¼Œæˆ‘å€‘é‚„æ˜¯æœƒæ ¹æ“šæœ€å¾Œçš„å¹³å‡å€¼ç•«ç·š)
        # ä½¿ç”¨ ffill() ç¢ºä¿å¦‚æœç•¶å‰ FCF æ˜¯è² çš„ï¼Œå®ƒæœƒå»¶ç”¨æœ€è¿‘ä¸€å€‹æ­£æ•¸çš„å¹³å‡å€æ•¸
        bands['mean'] = m_col.ffill() * val_col
        bands['up1'] = (m_col.ffill() + s_col.ffill()) * val_col
        bands['up2'] = (m_col.ffill() + 2 * s_col.ffill()) * val_col
        bands['down1'] = (m_col.ffill() - s_col.ffill()) * val_col
        bands['down2'] = (m_col.ffill() - 2 * s_col.ffill()) * val_col
        
        period_results[label] = bands
        
        # ç²å–æœ€å¾Œä¸€å€‹éç©ºå€¼ä½œç‚ºç•¶å‰å¹³å‡å€¼
        last_valid_mean = m_col.dropna().iloc[-1] if not m_col.dropna().empty else 0
        current_averages[label] = round(last_valid_mean, 2)

    return period_results, current_averages

def process_pipeline():
    """ åŸ·è¡Œä¸»ç¨‹åº """
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    for ticker in DOW_30:
        print(f"\nğŸ” [Step 1/5] Checking {ticker} status...")
        
        local_json = load_local_data(ticker)
        if local_json and len(local_json.get('data', [])) > 0:
            last_date_str = local_json['data'][-1]['date']
            last_date = datetime.strptime(last_date_str, "%Y-%m-%d")
            # åˆ¤æ–·æ˜¯å¦éœ€è¦æ›´æ–° (å¦‚æœæœ€å¾Œæ•¸æ“šæ—¥æœŸæ˜¯æ˜¨å¤©æˆ–æ›´æ—©)
            if last_date.date() >= datetime.now().date() - timedelta(days=1):
                print(f"  âœ… {ticker} is already up to date (Last: {last_date_str}). Skipping calculation.")
                continue

        print(f"  ğŸ“ˆ {ticker} needs update. Fetching 7-year price history from Yahoo Finance...")
        try:
            # ç²å– 7 å¹´æ•¸æ“šç¢ºä¿ 5Y æ»¾å‹•çª—æ ¼åœ¨èµ·å§‹é»æ˜¯æ»¿çš„
            full_price_df = yf.Ticker(ticker).history(period="7y")[['Close']]
            if full_price_df.empty:
                print(f"  âŒ No price data found for {ticker}")
                continue
            full_price_df.index = full_price_df.index.tz_localize(None)
        except Exception as e:
            print(f"  âŒ Yahoo Finance fetch failed: {e}")
            continue

        print(f"ğŸ§ª [Step 2/5] Fetching financial statements from FMP...")
        eps_df = get_income_statement(ticker)
        fcf_df = get_cash_flow_statement(ticker)

        # è¨ˆç®—ä¼°å€¼å¸¶
        print(f"ğŸ§® [Step 3/5] Calculating Multi-Period Valuation Bands (1Y, 2Y, 3Y, 5Y)...")
        # ä¿®æ”¹å‘¼å«è¡Œ
        pe_results, pe_avgs = calculate_multi_period_bands(ticker, full_price_df['Close'], eps_df['eps'], 'eps')
        fcf_results, fcf_avgs = calculate_multi_period_bands(ticker, full_price_df['Close'], fcf_df['fcf_ps'], 'fcf_ps')

        # æ•´ç†æ­·å²ç´€éŒ„è‡³ JSON æ ¼å¼
        print(f"ğŸ“¦ [Step 4/5] Packing historical data (Starting from 2021)...")
        history = []
        start_date = datetime(2021, 1, 1)
        output_df = full_price_df[full_price_df.index >= start_date]

        for date, row in output_df.iterrows():
            date_str = date.strftime("%Y-%m-%d")
            record = {
                "date": date_str,
                "price": round(row['Close'], 2),
                "valuation": {}
            }

            # éæ­·æ‰€æœ‰æ™‚é–“çª—æ ¼å¡«å¯«æ•¸æ“š
            for label in WINDOWS.keys():
                record["valuation"][label] = {}
                
                # PE æ¨¡å‹
                if label in pe_results and date in pe_results[label].index:
                    b = pe_results[label].loc[date]
                    record["valuation"][label]["pe"] = {k: round(v, 2) for k, v in b.to_dict().items()}
                
                # FCF æ¨¡å‹
                if label in fcf_results and date in fcf_results[label].index:
                    b = fcf_results[label].loc[date]
                    record["valuation"][label]["fcf"] = {k: round(v, 2) for k, v in b.to_dict().items()}
            
            history.append(record)

        # æœ€çµ‚ JSON å°è£
        output = {
            "ticker": ticker,
            "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "averages": {
                "pe": pe_avgs,
                "fcf": fcf_avgs
            },
            "data": history
        }

        print(f"ğŸ’¾ [Step 5/5] Saving results to {ticker}_valuation.json...")
        with open(os.path.join(OUTPUT_DIR, f"{ticker}_valuation.json"), "w") as f:
            json.dump(output, f)
        
        print(f"âœ¨ [Success] {ticker} pipeline completed.")

if __name__ == "__main__":
    print(f"ğŸš€ Starting Valuation Data Pipeline at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    process_pipeline()
    print(f"\nğŸ All tickers processed. Terminal standby.")