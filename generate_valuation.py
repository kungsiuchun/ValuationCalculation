import requests
import pandas as pd
import numpy as np
import yfinance as yf
import json
import os
import time
from datetime import datetime

# --- 1. é…ç½® ---
FMP_API_KEY = "F9dROu64FwpDqETGsu1relweBEoTcpID"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = os.path.join(BASE_DIR, "data")
CACHE_BASE_DIR = os.path.join(OUTPUT_DIR, "fmp_cache") # ç·©å­˜ä¸»ç›®éŒ„
DOW_30 = ["AAPL"]
## ["AMZN", "AAPL", "GOOGL", "MSFT", "WMT"] 

WINDOWS = {"1Y": 252, "2Y": 504, "3Y": 756, "5Y": 1260}
QUARTERS = ['q1', 'q2', 'q3', 'q4']

# --- 2. æŠ½å–å±¤ (Extract Layer) ---
def get_fmp_fragmented(endpoint, ticker):
    """
    [Data Engineering Logic]: 
    è‡ªå‹•å»ºç«‹å°æ‡‰ ticker çš„å­è³‡æ–™å¤¾ (ä¾‹å¦‚ fmp_cache/AMZN/)ã€‚
    """
    combined = []
    
    # å»ºç«‹ ticker å°ˆå±¬è·¯å¾‘ï¼šdata/fmp_cache/{ticker}
    ticker_cache_dir = os.path.join(CACHE_BASE_DIR, ticker.upper())
    os.makedirs(ticker_cache_dir, exist_ok=True) # è‡ªå‹•å»ºç«‹å¤šå±¤ç›®éŒ„

    for q in QUARTERS:
        # æ–‡ä»¶å‘½åä¿æŒ endpoint å€åˆ†
        cache_path = os.path.join(ticker_cache_dir, f"{endpoint}_{q}.json")
        
        # ç·©å­˜æª¢æŸ¥ (7å¤©æœ‰æ•ˆæœŸ)
        if os.path.exists(cache_path) and (time.time() - os.path.getmtime(cache_path)) < (7 * 86400):
            with open(cache_path, 'r') as f:
                combined.extend(json.load(f))
            continue

        url = f"https://financialmodelingprep.com/stable/{endpoint}/?symbol={ticker}&period={q}&apikey={FMP_API_KEY}"
        
        try:
            print(f"  ğŸš€ [API Call] Fetching {ticker} {endpoint} {q}...")
            res = requests.get(url).json()
            if isinstance(res, list):
                with open(cache_path, 'w') as f:
                    json.dump(res, f, indent=4) # å¢åŠ  indent æ–¹ä¾¿ DE é€²è¡Œ Debug
                combined.extend(res)
            time.sleep(0.2)
        except Exception as e:
            print(f"  âŒ [Error] Failed to fetch {endpoint} {q}: {e}")
            
    return combined

# --- 3. è½‰æ›å±¤ (Transform Layer) ---
def build_quarterly_ttm(ticker):
    inc_list = get_fmp_fragmented("income-statement", ticker)
    cf_list = get_fmp_fragmented("cash-flow-statement", ticker)
    ev_list = get_fmp_fragmented("enterprise-values", ticker)
    
    if not all([inc_list, cf_list, ev_list]): return None, None

    df_inc = pd.DataFrame(inc_list).drop_duplicates('date').set_index('date').sort_index()
    df_cf = pd.DataFrame(cf_list).drop_duplicates('date').set_index('date').sort_index()
    df_ev = pd.DataFrame(ev_list).drop_duplicates('date').set_index('date').sort_index()

    for df in [df_inc, df_cf, df_ev]:
        df.index = pd.to_datetime(df.index).tz_localize(None)

    # æ•¸æ“šåˆä½µèˆ‡è¨ˆç®— TTM
    df_inc['eps_ttm'] = df_inc['eps'].rolling(window=4).sum()
    df_main = pd.concat([df_inc[['eps_ttm']], df_cf['freeCashFlow'], df_ev['numberOfShares']], axis=1).ffill()
    df_main['fcf_ps_ttm'] = (df_main['freeCashFlow'] / df_main['numberOfShares']).rolling(window=4).sum()

    return df_main[['eps_ttm']].dropna(), df_main[['fcf_ps_ttm']].dropna()

def calculate_bands(ticker, prices_adj, metrics_df, col_name):
    # 1. çµ±ä¸€ç´¢å¼•æ ¼å¼ï¼ˆç¢ºä¿æ˜¯ DatetimeIndex ä¸”ç„¡æ™‚å€ï¼‰
    prices_adj.index = pd.to_datetime(prices_adj.index).tz_localize(None).normalize()
    metrics_df.index = pd.to_datetime(metrics_df.index).tz_localize(None).normalize()

    # 2. è™•ç† yfinance çš„é‡è¤‡æ•¸æ“š (AAPL å¸¸è¦‹å•é¡Œ)
    prices_adj = prices_adj[~prices_adj.index.duplicated(keep='first')]
    metrics_df = metrics_df[~metrics_df.index.duplicated(keep='first')]

    # 3. å‰µå»ºä¸€å€‹ã€Œå…¨æ™‚é–“è»¸ã€ï¼ˆåŒ…å«æ‰€æœ‰äº¤æ˜“æ—¥èˆ‡è²¡å ±æ—¥ï¼‰
    # é€™æ˜¯è§£æ±º "0 å€‹æ—¥æœŸå°é½Š" çš„é—œéµ
    all_dates = prices_adj.index.union(metrics_df.index).sort_values()
    
    # å»ºç«‹ä¸»è¡¨
    df = pd.DataFrame(index=all_dates)
    
    # 4. æ”¾å…¥åƒ¹æ ¼èˆ‡æŒ‡æ¨™
    df['price_adj'] = prices_adj  # åªæœ‰äº¤æ˜“æ—¥æœ‰å€¼
    df['metric_raw'] = metrics_df[col_name]  # åªæœ‰è²¡å ±æ—¥æœ‰å€¼
    
    # 5. ã€æ ¸å¿ƒæ­¥é©Ÿã€‘æ™‚é–“æ’å€¼ (Time-based Interpolation)
    # é€™æ¨£è²¡å ±æ—¥çš„æ•¸æ“šæœƒå¹³æ»‘åœ°åˆ†é…åˆ°äº¤æ˜“æ—¥ä¸Š
    df['metric_filled'] = df['metric_raw'].interpolate(method='time').ffill().bfill()
    
    # 6. å›åˆ‡åˆ°ã€Œåªæœ‰åƒ¹æ ¼äº¤æ˜“æ—¥ã€çš„è¡Œï¼Œç¢ºä¿è¼¸å‡ºçµæœé•·åº¦ä¸€è‡´
    df = df.loc[prices_adj.index].copy()

    # 7. è¨ˆç®—å€æ•¸ (æ­¤æ™‚å·²å®Œç¾å°é½Š)
    # æ³¨æ„ï¼šå¦‚æœåŸæœ¬è…³æœ¬ä¸­çš„ metric æ˜¯ raw EPSï¼Œæˆ‘å€‘ç›´æ¥ç®— PE
    df['multiple'] = df['price_adj'] / df['metric_filled'].replace(0, np.nan)
    
    # æ¸…ç†æ¥µç«¯å€¼
    df['multiple'] = df['multiple'].replace([np.inf, -np.inf], np.nan).ffill().bfill()

    results = {}
    avgs = {}

    for label, window in WINDOWS.items():
        # è¨ˆç®—æ»¾å‹•å¹³å‡
        m_col = df['multiple'].rolling(window=window, min_periods=20).mean()
        s_col = df['multiple'].rolling(window=window, min_periods=20).std().fillna(0)

        # ç”Ÿæˆè»Œé“
        res = pd.DataFrame(index=df.index)
        res['mean'] = m_col * df['metric_filled']
        res['up1'] = (m_col + s_col) * df['metric_filled']
        res['up2'] = (m_col + 2 * s_col) * df['metric_filled']
        res['down1'] = (m_col - s_col) * df['metric_filled']
        res['down2'] = (m_col - 2 * s_col) * df['metric_filled']

        results[label] = res.ffill().bfill().round(2)
        
        valid_m = m_col.dropna()
        avgs[label] = round(float(valid_m.iloc[-1]), 2) if not valid_m.empty else 0

    return results, avgs

def debug_valuation(ticker):
    print(f"\nğŸ” --- Deep Dive Debug: {ticker} ---")
    
    # 1. ç²å–åƒ¹æ ¼
    tk = yf.Ticker(ticker)
    hist = tk.history(period="7y", auto_adjust=False)
    # yfinance é»˜èªè¿”å›çš„å¯èƒ½æ˜¯ Adj Close ä½œç‚º Closeï¼Œæˆ‘å€‘å¼·åˆ¶æ‹¿é€™å…©å€‹
    df_prices = hist[['Close', 'Adj Close']].copy()
    df_prices.index = pd.to_datetime(df_prices.index).tz_localize(None).normalize()
    df_prices = df_prices[~df_prices.index.duplicated(keep='first')]

    # 2. ç²å–æŒ‡æ¨™ (å¾ä½ çš„ build_quarterly_ttm)
    eps_ttm, _ = build_quarterly_ttm(ticker)
    if eps_ttm is None:
        print("âŒ Error: eps_ttm is None")
        return
    
    eps_df = eps_ttm.copy()
    eps_df.index = pd.to_datetime(eps_df.index).tz_localize(None).normalize()

    # 3. åˆä½µè§€å¯Ÿ
    df = df_prices.join(eps_df, how='left')
    
    print("\n[Table 1: åŸå§‹æ•¸æ“šåˆä½µæƒ…æ³ (å‰ 5 è¡Œ)]")
    # æª¢æŸ¥ eps_ttm æ˜¯å¦æˆåŠŸ join é€²ä¾†ï¼Œé‚„æ˜¯å…¨æ˜¯ NaN
    print(df[['Close', 'Adj Close', 'eps_ttm']].head(5))

    # 4. æ¨¡æ“¬æ’å€¼
    df['eps_filled'] = df['eps_ttm'].interpolate(method='time').ffill()
    
    # 5. è¨ˆç®—é—œéµæ¯”ä¾‹ (é€™æ˜¯ç‚ºäº†é¿é–‹æ‹†åˆ†)
    # AAPL 2020å¹´ 1:4 æ‹†åˆ†ï¼Œé‚£æ™‚çš„ Adj Close / Close æ‡‰è©²ç´„ç­‰æ–¼ 0.25
    df['adj_ratio'] = df['Adj Close'] / df['Close']
    df['eps_final'] = df['eps_filled'] * df['adj_ratio']
    
    print("\n[Table 2: æ‹†åˆ†èª¿æ•´æª¢æŸ¥ (2020å¹´8æœˆæ‹†åˆ†å‰å¾Œ)]")
    # æ‰¾å‡º 2020-08-31 é™„è¿‘çš„æ•¸æ“šï¼Œçœ‹çœ‹ adj_ratio æœ‰æ²’æœ‰èµ·ä½œç”¨
    split_date = '2020-08-31'
    if split_date in df.index:
        loc = df.index.get_loc(split_date)
        print(df[['Close', 'Adj Close', 'adj_ratio', 'eps_final']].iloc[loc-2:loc+3])
    else:
        print(df[['Close', 'Adj Close', 'adj_ratio', 'eps_final']].tail(5))

    # 6. è¨ˆç®—å€æ•¸
    df['pe_ratio'] = df['Adj Close'] / df['eps_final'].replace(0, np.nan)
    
    print("\n[Table 3: æœ€çµ‚ PE è¨ˆç®—çµæœ]")
    print(df[['Adj Close', 'eps_final', 'pe_ratio']].tail(10))

    if df['pe_ratio'].isna().all():
        print("\nâŒ è­¦å ±ï¼šPE Ratio å…¨ä¿‚ NaNï¼")
        print(f"åŸå› æª¢æŸ¥ï¼š\n- eps_final æ˜¯å¦å…¨ç‚º 0? { (df['eps_final']==0).all() }")
        print(f"- eps_ttm æ˜¯å¦æ ¹æœ¬æ²’å°é½Šæ—¥æœŸ? { eps_df.index.isin(df_prices.index).sum() } å€‹æ—¥æœŸå°é½Š")



# --- 5. ä¸»ç¨‹åº ---
def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # å‘¼å« Debug
    ## debug_valuation("AAPL")

    for ticker in DOW_30:
        print(f"\nğŸ—ï¸  Pipeline Starting: {ticker}")
        prices = yf.Ticker(ticker).history(period="8y")[['Close']]
        prices.index = prices.index.tz_localize(None)

        eps_ttm, fcf_ttm = build_quarterly_ttm(ticker)
        if eps_ttm is None: continue

        pe_res, pe_avgs = calculate_bands(ticker, prices['Close'], eps_ttm, 'eps_ttm')
        fcf_res, fcf_avgs = calculate_bands(ticker, prices['Close'], fcf_ttm, 'fcf_ps_ttm')

        history = []
        for date, row in prices[prices.index >= '2021-01-01'].iterrows():
            if date not in pe_res["1Y"].index: continue
            history.append({
                "date": date.strftime("%Y-%m-%d"),
                "price": round(row['Close'], 2),
                "valuation": {
                    lb: {
                        "pe": pe_res[lb].loc[date].round(2).to_dict(),
                        "fcf": fcf_res[lb].loc[date].round(2).to_dict()
                    } for lb in WINDOWS
                }
            })

        # æœ€å¾Œçµæœä¹Ÿå­˜å…¥ ticker è³‡æ–™å¤¾
        final_dir = os.path.join(OUTPUT_DIR, "results", ticker.upper())
        os.makedirs(final_dir, exist_ok=True)
        
        with open(os.path.join(final_dir, "valuation_summary.json"), "w") as f:
            json.dump({"ticker": ticker, "averages": {"pe": pe_avgs, "fcf": fcf_avgs}, "data": history}, f, indent=4)
        print(f"âœ¨ [Success] {ticker} pipeline execution completed. Folder: {final_dir}")

if __name__ == "__main__":
    main()